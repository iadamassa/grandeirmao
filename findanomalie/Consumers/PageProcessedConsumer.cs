
using CheckAnomaliaApi.Models;
using CheckAnomaliaApi.Services;
using CrawlerWebApi.Models;
using MassTransit;

namespace CheckAnomaliaApi.Consumers;

public class PageProcessedConsumer(
  IPublishEndpoint _publishEndpoint,
  IHasAnomalieService _hasAnomalieService,
  ILogger<PageProcessedConsumer> _logger
) : IConsumer<PageProcessedMessage>
{

    public async Task Consume(ConsumeContext<PageProcessedMessage> context)
    {
        var message = context.Message;

        _logger.LogInformation("üìÑ Recebida p√°gina processada: {Url}", message.Url);
        _logger.LogInformation("üè∑Ô∏è  T√≠tulo: {Title}", message.Title);
        _logger.LogInformation("üìä Tamanho: {Size} chars, Links: {Links}",
            message.ContentSize, message.InternalLinksCount);

        try
        {
            var analysisResult = await _hasAnomalieService.AnalyzePageAsync(message);

            foreach (var item in analysisResult)
            {
                var l =  new LinkAnalysedMessage
                {
                    // Mapeie as propriedades conforme necess√°rio
                    CrawlRequest = item.CrawlRequest,
                    Analysis = item.Analysis,
                    SubjectsResearched = item.SubjectsResearched,
                    AnalyzedAt = item.AnalyzedAt,
                    HasAnomalie = item.HasAnomalie,
                    SuccessAnalyze = item.SuccessAnalyze,
                    UrlLink = message.Url,
                };

                await _publishEndpoint.Publish(l);

            }

            _logger.LogInformation("üîç An√°lise conclu√≠da para {Url}", message.Url);


        }
        catch (Exception ex)
        {
            _logger.LogError(ex, "‚ùå Erro ao analisar p√°gina {Url}", message.Url);
            throw;
        }
    }
}
