# Crawler Web API

Web API em .NET 8 que recebe mensagens do RabbitMQ via MassTransit para executar crawling de sites.

## Caracter√≠sticas

- ‚úÖ Processa apenas uma mensagem por vez (concorr√™ncia limitada)
- ‚úÖ Integra√ß√£o com RabbitMQ via MassTransit
- ‚úÖ Retry policy configurado para falhas
- ‚úÖ Crawler completo com extra√ß√£o de metadados
- ‚úÖ Logs detalhados do processamento
- ‚úÖ Simula√ß√£o de comportamento humano (delays, headers)
- ‚úÖ **Publica√ß√£o autom√°tica de p√°ginas processadas com sucesso**

## Pr√©-requisitos

- .NET 8 SDK
- RabbitMQ rodando (padr√£o: localhost:5672)

## Como executar

1. **Instalar depend√™ncias**:
```bash
dotnet restore
```

2. **Executar a aplica√ß√£o**:
```bash
dotnet run
```

3. **Verificar sa√∫de da API**:
```bash
curl http://localhost:5000/api/crawler/health
```

## Como usar

### Enviando mensagem via API (para testes)

```bash
curl -X POST http://localhost:5000/api/crawler/crawl \
  -H "Content-Type: application/json" \
  -d '{"url": "https://example.com"}'
```

### Enviando mensagem diretamente para RabbitMQ

Publique uma mensagem JSON na fila `crawl-requests`:

```json
{
  "url": "https://example.com",
  "requestedAt": "2024-01-01T00:00:00Z",
  "requestId": "guid-aqui"
}
```

## Filas do RabbitMQ

A aplica√ß√£o trabalha com **duas filas** principais:

### 1. Fila de Entrada: `crawl-requests`
**Prop√≥sito**: Recebe solicita√ß√µes de crawling  
**Formato da mensagem**:
```json
{
  "url": "https://example.com",
  "requestedAt": "2024-01-01T00:00:00Z",
  "requestId": "12345678-1234-1234-1234-123456789abc"
}
```

### 2. Fila de Sa√≠da: `page-processed` (Exchange autom√°tico do MassTransit)
**Prop√≥sito**: Recebe dados das p√°ginas processadas com sucesso  
**Nome da fila no RabbitMQ**: `CrawlerWebApi.Models:PageProcessedMessage`  
**Formato da mensagem**:
```json
{
  "url": "https://example.com/pagina",
  "title": "T√≠tulo da P√°gina",
  "htmlContent": "<html>...</html>",
  "metaDescription": "Descri√ß√£o da p√°gina extra√≠da do meta tag",
  "metaKeywords": "palavra1, palavra2, palavra3",
  "contentSize": 15420,
  "statusCode": 200,
  "contentType": "text/html",
  "internalLinksCount": 25,
  "crawledAt": "2024-01-01T10:30:00.000Z",
  "processedAt": "2024-01-01T10:30:05.250Z"
}
```

## Consumindo a fila de p√°ginas processadas

### Via RabbitMQ Management Interface
1. Acesse: `http://localhost:15672`
2. V√° em **Queues and Streams**
3. Procure pela fila: `CrawlerWebApi.Models:PageProcessedMessage`
4. Clique em **Get Messages** para visualizar

### Via MassTransit Consumer (C#)
```csharp
public class PageProcessedConsumer : IConsumer<PageProcessedMessage>
{
    private readonly ILogger<PageProcessedConsumer> _logger;

    public PageProcessedConsumer(ILogger<PageProcessedConsumer> logger)
    {
        _logger = logger;
    }

    public async Task Consume(ConsumeContext<PageProcessedMessage> context)
    {
        var message = context.Message;
        
        _logger.LogInformation("üìÑ Nova p√°gina processada: {Url}", message.Url);
        _logger.LogInformation("üè∑Ô∏è  T√≠tulo: {Title}", message.Title);
        _logger.LogInformation("üìä Tamanho: {Size} chars, Links: {Links}", 
                             message.ContentSize, message.InternalLinksCount);
        
        await ProcessPageContent(message);
    }

    private async Task ProcessPageContent(PageProcessedMessage page)
    {
        // Sua l√≥gica de processamento aqui
        await Task.CompletedTask;
    }
}
```

 
## Configura√ß√£o

### appsettings.json

```json
{
  "ConnectionStrings": {
    "RabbitMQ": "amqp://usuario:senha@localhost:5672/"
  }
}
```

### Vari√°veis de ambiente

- `ConnectionStrings__RabbitMQ`: String de conex√£o do RabbitMQ

## Estrutura do projeto

```
‚îú‚îÄ‚îÄ Controllers/
‚îÇ   ‚îî‚îÄ‚îÄ CrawlerController.cs           # API para testes
‚îú‚îÄ‚îÄ Consumers/
‚îÇ   ‚îî‚îÄ‚îÄ CrawlRequestConsumer.cs        # Consumer MassTransit
‚îú‚îÄ‚îÄ Models/
‚îÇ   ‚îú‚îÄ‚îÄ CrawlRequest.cs                # Modelo da mensagem de entrada
‚îÇ   ‚îú‚îÄ‚îÄ CrawlResult.cs                 # Resultado do crawling
‚îÇ   ‚îú‚îÄ‚îÄ PageData.cs                    # Dados de uma p√°gina
‚îÇ   ‚îî‚îÄ‚îÄ PageProcessedMessage.cs        # Mensagem de p√°gina processada
‚îú‚îÄ‚îÄ Services/
‚îÇ   ‚îú‚îÄ‚îÄ ICrawlerService.cs             # Interface do servi√ßo
‚îÇ   ‚îî‚îÄ‚îÄ CrawlerService.cs              # Implementa√ß√£o do crawler
‚îú‚îÄ‚îÄ Program.cs                         # Configura√ß√£o da aplica√ß√£o
‚îî‚îÄ‚îÄ appsettings.json                  # Configura√ß√µes
```

## Funcionamento

1. A aplica√ß√£o se conecta ao RabbitMQ e fica ouvindo a fila `crawl-requests`
2. Quando recebe uma mensagem com uma URL, inicia o crawling
3. O sem√°foro garante que apenas uma opera√ß√£o de crawling aconte√ßa por vez
4. **Para cada p√°gina processada com sucesso**, uma mensagem √© publicada na fila `page-processed`
5. A mensagem cont√©m URL, t√≠tulo, conte√∫do HTML e metadados extra√≠dos
6. Em caso de erro, utiliza a pol√≠tica de retry configurada

## Dados da mensagem PageProcessedMessage

| Campo | Tipo | Descri√ß√£o |
|-------|------|-----------|
| `url` | string | URL da p√°gina processada |
| `title` | string | T√≠tulo extra√≠do da tag `<title>` |
| `htmlContent` | string | Conte√∫do HTML completo da p√°gina |
| `metaDescription` | string | Conte√∫do da meta tag description |
| `metaKeywords` | string | Conte√∫do da meta tag keywords |
| `contentSize` | int | Tamanho do HTML em caracteres |
| `statusCode` | int | Status HTTP da resposta (200, 404, etc.) |
| `contentType` | string | Content-Type da resposta |
| `internalLinksCount` | int | Quantidade de links internos encontrados |
| `crawledAt` | DateTime | Data/hora da extra√ß√£o da p√°gina |
| `processedAt` | DateTime | Data/hora do envio da mensagem |

## Logs

A aplica√ß√£o gera logs detalhados sobre:
- Recebimento de mensagens
- Progresso do crawling
- **Envio de p√°ginas para fila de processamento**
- Estat√≠sticas finais
- Erros e tentativas de retry

## Casos de uso para a fila de p√°ginas processadas

- **Indexa√ß√£o**: Enviar conte√∫do para Elasticsearch ou Solr
- **An√°lise de conte√∫do**: Processar HTML com IA/ML
- **Armazenamento**: Salvar p√°ginas em banco de dados
- **Notifica√ß√µes**: Alertar sobre p√°ginas espec√≠ficas
- **Backup**: Arquivar conte√∫do processado
- **Analytics**: Extrair m√©tricas e estat√≠sticas

## Personaliza√ß√£o

Para salvar os resultados em banco de dados ou enviar para outras filas/exchanges, modifique o m√©todo `ProcessSuccessfulPageAsync` no `CrawlerService` ou crie consumers espec√≠ficos para a fila `PageProcessedMessage`.
```
